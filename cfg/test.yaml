expr_name: "test_light_maib"
expr_group: "test"
log_dir: ""
seed: 0

framework:
  distributed:
    use: False
    auto_connect: True
    auto_copy: True
    nodes:
      master:
        ip: "127.0.0.1"
      workers:      
        - ip: "127.0.0.1"
  name: "psro"
  max_generations: 50

# TODO
league:
  role: "client"
  server_ip: "127.0.0.1"
  server_port: "10001"

rollout_manager:
  distributed:
    resouces:
      - ["node:${distributed.nodes.master.ip}",0.01]
  num_workers:
  seed: 10086
  saving_interval: 100 # the sequence of dump model's weight
  worker:
    distributed:
      resouces:
      - ["num_cpus",1.5] # suggestion: cannot start too much rollout workers...
    rollout_length: 3001
    sample_length: 1000
    envs:
      - cls: "gfootball"
        id_prefix: "gfootbal"
        scenario_config:
          env_name: "5_vs_5"
          number_of_left_players_agent_controls: 4
          number_of_right_players_agent_controls: 4
          representation: "raw"
          rewards: "scoring, checkpoints"
          stacked: False
          logdir: '/tmp/football/malib_psro'
          write_goal_dumps: False
          write_full_episode_dumps: False
          render: False
          other_config_options:
            action_set: default   #default/v2
        reward_config:
          win_reward: 0
          preprocess_score: 0
          ball_position_reward: 0
          yellow_reward: 5
          min_dist_reward: 0
          goal_reward: 5
          #pass_reward: 0.02
          shot_reward: 0
          lost_ball_reward: 0.
          player_move_reward: 0.
          dist_goal_to_line: 0
          role_based_r: 0
          pure_goal: 0
          pure_lose_goal: 0
    decaying_exploration:
      init_noise: 0             #random exploration noise level
      total_epoch_to_zero: 2000    #number of epoch when exploration noise decay to zero
      interval: 400                #number of epoch at each fixed exploration noise level

agent_manager:
  distributed:
    resources:
      - ["node:${distributed.nodes.master.ip}",0.01]
  num_agents: 2
  share_policies: True
  initial_policies:
    agent_0:
      # - policy_id: zs_one
      #   policy_dir: trained_models/5v5_jidi/zs_one
#      - policy_id: zs_two
#        policy_dir: trained_models/5v5_jidi/zs_two
      # - policy_id: MrPasserby
      #   policy_dir: trained_models/5v5/MrPasserby # the initial policy dir
      # - policy_id: built_in
      #   policy_dir: trained_models/5v5/built_in # the initial policy dir
      # - policy_id: defence_1
      #   policy_dir: trained_models/5v5/defence_1 # the initial policy dir
      # - policy_id: hyperion
      #   policy_dir: trained_models/5v5/hyperion # the initial policy dir
       - policy_id: imitation
         policy_dir: trained_models/5v5/imitation # the initial policy dir
      # - policy_id: defence_2
      #   policy_dir: trained_models/5v5/defence_2 # the initial policy dir
      # - policy_id: defence_3
      #   policy_dir: trained_models/5v5/defence_3
      # - policy_id: beat_defence_1
      #   policy_dir: trained_models/5v5/beat_defence_1
      # - policy_id: beat_counterattack3
      #   policy_dir: trained_models/5v5/beat_counterattack3
      # - policy_id: counter_attack3
      #   policy_dir: trained_models/5v5/counter_attack3 # the initial policy dir
      # - policy_id: passer_shot
      #   policy_dir: trained_models/5v5/passer_shot
  evaluation_manager:
    num_eval_rollouts: 25
  policy_data_manager:
    update_func: "gr_football" 
    fields:
      - key: "payoff"
        missing_value: -100 
      - key: "score"
        missing_value: -100 
      - key: "win"
        missing_value: -100 
      - key: "lose"
        missing_value: -100 
      - key: "my_goal"
        missing_value: -100 
      - key: "goal_diff"
        missing_value: -100 

training_manager:
  distributed:
    resouces:
      - ["node:${distributed.nodes.master.ip}",0.01]
  num_gpus:
  master_addr: "127.0.0.1"
  master_port:  "10997"
  batch_size: 128
  max_rollout_epoch: 1000
  num_prefetchers: 1
  local_queue_size: 1
  trainer:
    distributed:
      resouces:
        - ["node:${distributed.nodes.master.ip}",0.01]
        - ["num_gpus",1]
    # control the frequency of remote parameter update
    update_interval: 1
    # 64ä¸ªepisode
    batch_size: 450  # how many data sample from DatasetServer per time.
    optimizer: "Adam"
    actor_lr: 5.e-4
    critic_lr: 5.e-4
    opti_eps: 1.e-5
    weight_decay: 0.0
    lr_decay: False            #update_linear_schedule
    lr_decay_epoch: 2000      #how many rollout steps till zero

data_server:
  distributed:
    resources:
      - ["node:${distributed.nodes.master.ip}",0.01]
  episode_capacity: 1000
  sampler_type: "uniform"
  sample_max_usage: 15
  read_timeout: 120

policy_server:
  distributed:
    resources:
      - ["node:${distributed.nodes.master.ip}",0.01]

algorithm:
  name: "MAPPO"
  model_config:
    model: sgc
    initialization:
      use_orthogonal: True
      gain: 1.
    actor:
      network: mlp
      layers:
        - units: 256
          activation: ReLU
        - units: 128
          activation: ReLU
        - units: 64
          activation: ReLU
      output:
        activation: False
    critic:
      network: mlp
      layers:
        - units: 256
          activation: ReLU
        - units: 128
          activation: ReLU
        - units: 64
          activation: ReLU
      output:
        activation: False
  # set hyper parameter
  custom_config:
    gamma: 1.0
    use_cuda: False  # enable cuda or not
    use_q_head: False
    ppo_epoch: 5
    num_mini_batch: 1  # the number of mini-batches
    
    return_mode: new_gae
    gae:
      gae_lambda: 0.95
    vtrace:
      clip_rho_threshold: 1.0
      clip_pg_rho_threshold: 100.0

    use_rnn: False
    # this is not used, instead it is fixed to last hidden in actor/critic
    rnn_layer_num: 1
    rnn_data_chunk_length: 16

    use_feature_normalization: True
    use_popart: True
    popart_beta: 0.99999

    entropy_coef: 0.0005
    clip_param: 0.2

    use_modified_mappo: True

  policy_init_cfg:
    -  condition: "==0" # condition if True in order
       strategy: random # now support pretrained, inherit_last, random
#       policy_id: imitation
#       policy_dir: trained_models/5v5/imitation
       reset_layer:
       noise_layer:

    -  condition: "default" # default case.
       strategy: inherit_last
       reset_layer:
       noise_layer:
