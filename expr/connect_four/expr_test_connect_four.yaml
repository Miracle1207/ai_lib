expr_group: connect_four
expr_name: test_connect_four
log_dir: /root/expr/log
seed: 0
eval_only: False
# resume: # currently only support resume from a generation but not a step in generation
#   log_dir: 
#   overwrite: False # if overwrite

framework:
  distributed:
    use: False
    auto_connect:
    auto_copy:
    nodes:
      master:
        ip: auto
        port: "10001"
      workers:      
        - ip:
          port:
  name: "psro"
  max_generations: 50

monitor:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${framework.distributed.nodes.master.ip}",0.01]

# TODO
league:
  role: "client"
  server_ip: ""
  server_port: ""

rollout_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${framework.distributed.nodes.master.ip}",0.01]
  num_workers: 120
  seed: 12345
  saving_interval: 100 # the sequence of dump model's weight
  max_rollout_epoch: 10000
  worker:
    rollout_func_name: "rollout_func_aec"
    distributed:
      resources:
        num_cpus: 1
    rollout_length: 3001
    eval_rollout_length: 3001
    sample_length: 1000
    padding_length: 42 # 6*7 steps
    envs:
      - cls: "connect_four"
        id_prefix: "connect_four"
    decaying_exploration:
      init_noise: 0             #random exploration noise level
      total_epoch_to_zero: 2000    #number of epoch when exploration noise decay to zero
      interval: 400                #number of epoch at each fixed exploration noise level

agent_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${framework.distributed.nodes.master.ip}",0.01]
  num_agents: 2
  share_policies: True
  evaluation_manager:
    num_eval_rollouts: 100
  policy_data_manager:
    update_func: "connect_four" 
    fields:
      payoff:
        missing_value: -100 
      score:
        missing_value: -100 
      win:
        missing_value: -100
      lose:
        missing_value: -100

training_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${framework.distributed.nodes.master.ip}",0.01]
 
  master_addr: "127.0.0.1"
  master_port:  #"12774"
  local_queue_size: 1
  batch_size: 450 # how many data sample from DatasetServer per time.
  num_prefetchers: 1
  data_prefetcher:
    distributed:
      resources:
        num_cpus: 1
  num_trainers: 1
  gpu_preload: False
  # control the frequency of remote parameter update
  update_interval: 1
  trainer:
    distributed:
      resources:
        num_cpus: 1
        num_gpus: 1
        resources:
          - ["node:${framework.distributed.nodes.master.ip}",0.01]
    optimizer: "Adam"
    actor_lr: 5.e-4
    critic_lr: 5.e-4
    opti_eps: 1.e-5
    weight_decay: 0.0
    lr_decay: False            #update_linear_schedule
    lr_decay_epoch: 2000      #how many rollout steps till zero

data_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${framework.distributed.nodes.master.ip}",0.01]
  episode_capacity: 900
  sampler_type: "lulrf"
  sample_max_usage: 10000
  read_timeout: 120

policy_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${framework.distributed.nodes.master.ip}",0.01]

algorithm:
  name: "MAPPO"
  model_config:
    model: "connect_four.basic"
    initialization:
    actor:
    critic:

  # set hyper parameter
  custom_config:
    gamma: 1.0
    use_cuda: False  # enable cuda or not
    use_q_head: False
    ppo_epoch: 5
    num_mini_batch: 1  # the number of mini-batches
    
    return_mode: new_gae
    gae:
      gae_lambda: 0.95
    vtrace:
      clip_rho_threshold: 1.0
      clip_pg_rho_threshold: 100.0

    use_rnn: False
    # this is not used, instead it is fixed to last hidden in actor/critic
    rnn_layer_num: 1
    rnn_data_chunk_length: 16

    use_feature_normalization: True
    use_popart: True
    popart_beta: 0.99999

    entropy_coef: 0.000
    clip_param: 0.2

    use_modified_mappo: False

  policy_init_cfg:
    new_policy_ctr_start: 10
    init_cfg:
      - condition: "==11" # condition if True in order
        strategy: pretrained # now support pretrained, inherit_last, random
        policy_id: agent_0_MAPPO_11
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_11/best
        reset_layer:
    #           network_name: 'actor'
    #           layer_type: 'base'
    #           layer_idx: '1'     #linear layer index
        noise_layer:
          #  network_name: 'actor, critic'
          #  layer_type: 'base, out'
          #  layer_idx: '1,2'
          #  noise_scale: 0.1

      - condition: "==12" # default case.
        strategy: random # now support pretrained, inherit_last, random, inherit_last_best
        # policy_id: defence_1
        # policy_dir: trained_models/5v5/defence_1
        reset_layer:
    #           network_name: 'actor'
    #           layer_type: 'base'
    #           layer_idx: 1     #linear layer index
        noise_layer:
          #  network_name: 'actor'
          #  layer_type: 'base, out'
          #  layer_idx: '1,2'
          #  noise_scale: 0.1


      - condition: "default" # default case.
        strategy: inherit_last_best # now support pretrained, inherit_last, random, inherit_last_best
        # policy_id: defence_1
        # policy_dir: trained_models/5v5/defence_1
        reset_layer:
    #           network_name: 'actor'
    #           layer_type: 'base'
    #           layer_idx: 1     #linear layer index
        noise_layer:
          #  network_name: 'actor'
          #  layer_type: 'base, out'
          #  layer_idx: '1,2'
          #  noise_scale: 0.1

  initial_policies:
    agent_0:
      - policy_id: agent_0_MAPPO_1
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_1/best
      - policy_id: agent_0_MAPPO_2
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_2/best
      - policy_id: agent_0_MAPPO_3
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_3/best
      - policy_id: agent_0_MAPPO_4
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_4/best
      - policy_id: agent_0_MAPPO_5
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_5/best
      - policy_id: agent_0_MAPPO_6
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_6/best
      - policy_id: agent_0_MAPPO_7
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_7/best
      - policy_id: agent_0_MAPPO_8
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_8/best
      - policy_id: agent_0_MAPPO_9
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_9/best
      - policy_id: agent_0_MAPPO_10
        policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_10/best
      # - policy_id: agent_0_MAPPO_11
      #   policy_dir: /root/async_gfootball/expr/log/connect_four/test_connect_four/2022-09-03-23-40-31/agent_0/agent_0_MAPPO_11/best
      # - policy_id: zs_one
      #   policy_dir: trained_models/5v5_jidi/zs_one
      # - policy_id: zs_two
      #   policy_dir: trained_models/5v5_jidi/zs_two
      # - policy_id: MrPasserby
      #   policy_dir: trained_models/5v5/MrPasserby # the initial policy dir
      # - policy_id: built_in
      #   policy_dir: trained_models/5v5/built_in # the initial policy dir
      # - policy_id: defence_1
      #   policy_dir: trained_models/5v5/defence_1 # the initial policy dir
      # - policy_id: hyperion
      #   policy_dir: trained_models/5v5/hyperion # the initial policy dir
      # - policy_id: imitation
      #   policy_dir: trained_models/5v5/imitation # the initial policy dir
      # - policy_id: defence_2
      #   policy_dir: trained_models/5v5/defence_2 # the initial policy dir
      # - policy_id: defence_3
      #   policy_dir: trained_models/5v5/defence_3
      # - policy_id: beat_defence_1
      #   policy_dir: trained_models/5v5/beat_defence_1
      # - policy_id: beat_counterattack3
      #   policy_dir: trained_models/5v5/beat_counterattack3
      # - policy_id: counter_attack3
      #   policy_dir: trained_models/5v5/counter_attack3 # the initial policy dir
      # - policy_id: passer_shot
      #   policy_dir: trained_models/5v5/passer_shot