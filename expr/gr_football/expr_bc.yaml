expr_group: gr_football
expr_name: behavior_cloning_test
log_dir: /root/expr/log
seed: 0
eval_only: False

distributed:
  use: False
  auto_connect:
  auto_copy:
  nodes:
    master:
      ip: auto
    workers:      
      - ip:

framework:
  name: "psro"
  max_generations: 50
  meta_solver: "uniform"

agent_manager:
  num_agents: 2
  share_policies: True

evaluation_manager:
  num_eval_rollouts: 1

policy_data_manager:
  update_func: "gr_football" 
  fields:
    payoff:
      missing_value: -100 
    score:
      missing_value: -100 
    win:
      missing_value: -100 
    lose:
      missing_value: -100 
    my_goal:
      missing_value: -100 
    goal_diff:
      missing_value: -100 
        
monitor:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

# TODO
league:
  role: "client"
  server_ip: ""
  server_port: ""

rollout_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  num_workers: 100
  seed: 12345
  saving_interval: 100 # the sequence of dump model's weight
  max_rollout_epoch: 10000
  worker:
    distributed:
      resources:
        num_cpus: 1
    rollout_length: 3001
    eval_rollout_length: 3001
    sample_length: 1000
    padding_length: # of not use in gr_football
    rollout_func_name: "rollout_func"
    envs:
      - cls: "gr_football"
        id_prefix: "gr_footbal"
        scenario_config:
          env_name: "10_vs_10_kaggle"
          number_of_left_players_agent_controls: 1
          number_of_right_players_agent_controls: 10
          representation: "raw"
          rewards: "scoring, checkpoints"
          stacked: False
          logdir: '/tmp/football/malib_psro'
          write_goal_dumps: False
          write_full_episode_dumps: False
          render: False
          other_config_options:
            action_set: v2   #default/v2
        reward_config:
          win_reward: 0
          preprocess_score: 0
          ball_position_reward: 0
          yellow_reward: 0
          min_dist_reward: 0
          goal_reward: 0
          #pass_reward: 0.02
          shot_reward: 0
          lost_ball_reward: 0.
          player_move_reward: 0.
          dist_goal_to_line: 0
          role_based_r: 5
          pure_goal: 0
          pure_lose_goal: 0
    decaying_exploration:
      init_noise: 0             #random exploration noise level
      total_epoch_to_zero: 2000    #number of epoch when exploration noise decay to zero
      interval: 400                #number of epoch at each fixed exploration noise level

training_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
 
  master_addr: "127.0.0.1"
  master_port:  #"12774"
  local_queue_size: 1
  batch_size: 
    expert_data: 1200000 # how many data sample from DatasetServer per time.
  num_prefetchers: 1
  data_prefetcher:
    distributed:
      resources:
        num_cpus: 1
  num_trainers: 6
  # control the frequency of remote parameter update
  update_interval: 1
  gpu_preload: False
  trainer:
    distributed:
      resources:
        num_cpus: 1
        num_gpus: 1
        resources:
          - ["node:${distributed.nodes.master.ip}",0.01]
    optimizer: "Adam"
    actor_lr: 5.e-4
    critic_lr: 5.e-4
    opti_eps: 1.e-5
    weight_decay: 0.0
    lr_decay: False            #update_linear_schedule
    lr_decay_epoch: 2000      #how many rollout steps till zero

data_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  episode_capacity: 900
  sampler_type: "lumrf"
  sample_max_usage: 100000000
  read_timeout: 120

shared_data_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  episode_capacity: 3000000
  sampler_type: "lumrf"
  sample_max_usage: 100000000
  read_timeout: 120
  preload_data_path: "/root/autodl-tmp/data/saltyfish_dataset_1_bc_data_basic.pkl"

policy_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

populations:
  - population_id: default # population_id
    algorithm:
      name: "MAPPO"
      model_config:
        model: "gr_football.basic_11"
        initialization:
          use_orthogonal: True
          gain: 1.
        actor:
          network: mlp
          layers:
            - units: 256
              activation: ReLU
            - units: 128
              activation: ReLU
            - units: 64
              activation: ReLU
          output:
            activation: False
        critic:
          network: mlp
          layers:
            - units: 256
              activation: ReLU
            - units: 128
              activation: ReLU
            - units: 64
              activation: ReLU
          output:
            activation: False

      # set hyper parameter
      custom_config:
        gamma: 0.9995
        use_cuda: False  # enable cuda or not
        use_q_head: False
        ppo_epoch: 5
        num_mini_batch: 1  # the number of mini-batches
        
        return_mode: new_gae
        gae:
          gae_lambda: 0.95
        vtrace:
          clip_rho_threshold: 1.0
          clip_pg_rho_threshold: 100.0

        use_rnn: False
        # this is not used, instead it is fixed to last hidden in actor/critic
        rnn_layer_num: 1
        rnn_data_chunk_length: 16

        use_feature_normalization: True
        use_popart: True
        popart_beta: 0.99999

        entropy_coef: 0.00
        clip_param: 0.2

        use_modified_mappo: False

      policy_init_cfg:
        agent_0: # agent_id
          new_policy_ctr_start: -1
          init_cfg:
            - condition: "==0" # condition if True in order
              strategy: random # now support pretrained, inherit_last, random
              # policy_id: imitation
              # policy_dir: trained_models/5v5/imitation # the initial policy dir
              reset_layer:
          #           network_name: 'actor'
          #           layer_type: 'base'
          #           layer_idx: '1'     #linear layer index
              noise_layer:
                #  network_name: 'actor, critic'
                #  layer_type: 'base, out'
                #  layer_idx: '1,2'
                #  noise_scale: 0.1

            - condition: "default" # default case.
              strategy: inherit_last_best # now support pretrained, inherit_last, random
              # policy_id: defence_1
              # policy_dir: trained_models/5v5/defence_1
              reset_layer:
          #           network_name: 'actor'
          #           layer_type: 'base'
          #           layer_idx: 1     #linear layer index
              noise_layer:
                #  network_name: 'actor'
                #  layer_type: 'base, out'
                #  layer_idx: '1,2'
                #  noise_scale: 0.1

          initial_policies:
            - policy_id: built_in
              policy_dir: light_malib/trained_models/gr_football/11_vs_11/built_in # the initial policy dir
            # - policy_id: MAPPO_0
            #   policy_dir: trained_models/5v5/imitation # the initial policy dir
            # - policy_id: MAPPO_1
            #   policy_dir: expr/log/gr_football/imitation_psro/2022-08-30-22-17-56/agent_0/MAPPO_1/best
            # - policy_id: MAPPO_2
            #   policy_dir: expr/log/gr_football/imitation_psro/2022-08-30-22-17-56/agent_0/MAPPO_2/best
            # - policy_id: MAPPO_3
            #   policy_dir: expr/log/gr_football/imitation_psro/2022-08-31-09-07-07/agent_0/MAPPO_1/best
            # - policy_id: MAPPO_4
            #   policy_dir: expr/log/gr_football/imitation_psro/2022-08-31-12-26-50/agent_0/MAPPO_4/best
            # - policy_id: MAPPO_5
            #   policy_dir: expr/log/gr_football/imitation_psro/2022-08-31-12-26-50/agent_0/MAPPO_5/best
            # - policy_id: zs_one
            #   policy_dir: trained_models/5v5_jidi/zs_one
            # - policy_id: zs_two
            #   policy_dir: trained_models/5v5_jidi/zs_two
            # - policy_id: MrPasserby
            #   policy_dir: trained_models/5v5/MrPasserby # the initial policy dir
            # - policy_id: built_in
            #   policy_dir: trained_models/5v5/built_in # the initial policy dir
            # - policy_id: defence_1
            #   policy_dir: trained_models/5v5/defence_1 # the initial policy dir
            # - policy_id: hyperion
            #   policy_dir: trained_models/5v5/hyperion # the initial policy dir
            # - policy_id: imitation
            #   policy_dir: trained_models/5v5/imitation # the initial policy dir
            # - policy_id: defence_2
            #   policy_dir: trained_models/5v5/defence_2 # the initial policy dir
            # - policy_id: defence_3
            #   policy_dir: trained_models/5v5/defence_3
            # - policy_id: beat_defence_1
            #   policy_dir: trained_models/5v5/beat_defence_1
            # - policy_id: beat_counterattack3
            #   policy_dir: trained_models/5v5/beat_counterattack3
            # - policy_id: counter_attack3
            #   policy_dir: trained_models/5v5/counter_attack3 # the initial policy dir
            # - policy_id: passer_shot
            #   policy_dir: trained_models/5v5/passer_shot